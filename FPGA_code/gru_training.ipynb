{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from kernel import run_kernel\n",
    "from hardata import HarData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    A class for reading and preprocessing text data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str, sequence_length: int):\n",
    "        \"\"\"\n",
    "        Initializes a DataReader object with the path to a text file and the desired sequence length.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the text file.\n",
    "            sequence_length (int): The length of the sequences that will be fed to the self.\n",
    "        \"\"\"\n",
    "        with open(path) as f:\n",
    "            # Read the contents of the file\n",
    "            self.data = f.read()\n",
    "\n",
    "        # Find all unique characters in the text\n",
    "        chars = list(set(self.data))\n",
    "\n",
    "        # Create dictionaries to map characters to indices and vice versa\n",
    "        self.char_to_idx = {ch: i for (i, ch) in enumerate(chars)}\n",
    "        self.idx_to_char = {i: ch for (i, ch) in enumerate(chars)}\n",
    "\n",
    "        # Store the size of the text data and the size of the vocabulary\n",
    "        self.data_size = len(self.data)\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "        # Initialize the pointer that will be used to generate sequences\n",
    "        self.pointer = 0\n",
    "\n",
    "        # Store the desired sequence length\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        Generates a batch of input and target sequences.\n",
    "\n",
    "        Returns:\n",
    "            inputs_one_hot (np.ndarray): A numpy array with shape `(batch_size, vocab_size)` where each row is a one-hot encoded representation of a character in the input sequence.\n",
    "            targets (list): A list of integers that correspond to the indices of the characters in the target sequence, which is the same as the input sequence shifted by one position to the right.\n",
    "        \"\"\"\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.sequence_length\n",
    "\n",
    "        # Get the input sequence as a list of integers\n",
    "        inputs = [self.char_to_idx[ch] for ch in self.data[input_start:input_end]]\n",
    "\n",
    "        # One-hot encode the input sequence\n",
    "        inputs_one_hot = np.zeros((len(inputs), self.vocab_size))\n",
    "#         print('batch_size:', (len(inputs)))\n",
    "        inputs_one_hot[np.arange(len(inputs)), inputs] = 1\n",
    "\n",
    "        # Get the target sequence as a list of integers\n",
    "        targets = [self.char_to_idx[ch] for ch in self.data[input_start + 1:input_end + 1]]\n",
    "\n",
    "        # Update the pointer\n",
    "        self.pointer += self.sequence_length\n",
    "\n",
    "        # Reset the pointer if the next batch would exceed the length of the text data\n",
    "        if self.pointer + self.sequence_length + 1 >= self.data_size:\n",
    "            self.pointer = 0\n",
    "\n",
    "        return inputs_one_hot, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def cut_in_sequences(x,y,seq_len,inc=1):\n",
    "\n",
    "    sequences_x = []\n",
    "    sequences_y = []\n",
    "\n",
    "    for s in range(0,x.shape[0] - seq_len,inc):\n",
    "        start = s\n",
    "        end = start+seq_len\n",
    "        sequences_x.append(x[start:end])\n",
    "        sequences_y.append(y[start:end])\n",
    "\n",
    "    return np.stack(sequences_x,axis=1),np.stack(sequences_y,axis=1)\n",
    "\n",
    "\n",
    "numID = 13\n",
    "\n",
    "\n",
    "class HarData:\n",
    "\n",
    "    def __init__(self,seq_len=16):\n",
    "        print(\"Parsing for Patient File {}\".format(numID))\n",
    "\n",
    "        train_x = np.loadtxt(f\"Train_570_{numID}_glucose.txt\")\n",
    "        train_y = (np.loadtxt(f\"Train_570_{numID}_glucose.txt\") - 1)  # .astype(np.int32)\n",
    "        train_ins = np.loadtxt(f\"Train_570_{numID}_bolus.txt\")\n",
    "        train_meal = np.loadtxt(f\"Train_570_{numID}_meal.txt\")\n",
    "        train_basal = np.loadtxt(f\"Train_570_{numID}_basal.txt\")\n",
    "        train_initIsc1 = np.loadtxt(f\"Train_570_{numID}_Isc1.txt\")\n",
    "        train_initIsc2 = np.loadtxt(f\"Train_570_{numID}_Isc2.txt\")\n",
    "        train_initIp = np.loadtxt(f\"Train_570_{numID}_Ip.txt\")\n",
    "        train_BW = np.loadtxt(f\"Train_570_{numID}_BW.txt\")\n",
    "        train_u2ss = np.loadtxt(f\"Train_570_{numID}_u2ss.txt\")\n",
    "\n",
    "        \n",
    "\n",
    "        train_meal2 = train_meal\n",
    "\n",
    "        test_x = np.loadtxt(f\"Train_570_{numID}_glucose.txt\")\n",
    "        test_y = np.loadtxt(f\"Train_570_{numID}_glucose.txt\") - 1  # .astype(np.int32)\n",
    "        test_ins = np.loadtxt(f\"Train_570_{numID}_bolus.txt\")\n",
    "        test_meal = np.loadtxt(f\"Train_570_{numID}_meal.txt\")\n",
    "        test_meal2 = test_meal\n",
    "        test_basal = np.loadtxt(f\"Train_570_{numID}_basal.txt\")\n",
    "        test_initIsc1 = np.loadtxt(f\"Train_570_{numID}_Isc1.txt\")\n",
    "        test_initIsc2 = np.loadtxt(f\"Train_570_{numID}_Isc2.txt\")\n",
    "        test_initIp = np.loadtxt(f\"Train_570_{numID}_Ip.txt\")\n",
    "        test_BW = np.loadtxt(f\"Train_570_{numID}_BW.txt\")\n",
    "        test_u2ss = np.loadtxt(f\"Train_570_{numID}_u2ss.txt\")\n",
    "\n",
    "        \n",
    "        # shape = tf.shape(test_basal)\n",
    "        # with tf.compat.v1.Session() as sess:\n",
    "        #     numpy_number = sess.run(shape[1])\n",
    "        # Nloop = numpy_number\n",
    "        # print(\"Nloop {}\".format(Nloop))\n",
    "        train_x,train_y = cut_in_sequences(train_x,train_y,seq_len)\n",
    "        train_ins,train_meal = cut_in_sequences(train_ins,train_meal,seq_len)\n",
    "        train_basal,train_initIsc1 = cut_in_sequences(train_basal,train_initIsc1,seq_len)\n",
    "        train_initIsc2, train_initIp = cut_in_sequences(train_initIsc2, train_initIp, seq_len)\n",
    "        train_BW, train_u2ss = cut_in_sequences(train_BW, train_u2ss, seq_len)\n",
    "\n",
    "        test_x,test_y = cut_in_sequences(test_x,test_y,seq_len,inc=8)\n",
    "        test_ins,test_meal = cut_in_sequences(test_ins,test_meal,seq_len,inc=8)\n",
    "\n",
    "        test_basal, test_initIsc1 = cut_in_sequences(test_basal, test_initIsc1, seq_len,inc=8)\n",
    "        test_initIsc2, test_initIp = cut_in_sequences(test_initIsc2, test_initIp, seq_len,inc=8)\n",
    "        test_BW, test_u2ss = cut_in_sequences(test_BW, test_u2ss, seq_len,inc=8)\n",
    "        print(\"Total number of testing sequences: {}\".format(test_initIsc1.shape[1]))\n",
    "        #permutation = np.random.RandomState(893429).permutation(train_x.shape[1])\n",
    "        valid_size = int(0.1*train_x.shape[1])\n",
    "        print(\"Validation split: {}, training split: {}\".format(valid_size,train_x.shape[1]-valid_size))\n",
    "\n",
    "        self.valid_x = train_x[:,:valid_size]\n",
    "        self.valid_ins = train_ins[:,:valid_size]\n",
    "        self.valid_meal = train_meal[:,:valid_size]\n",
    "        self.valid_y = train_y[:,:valid_size]\n",
    "        self.valid_basal = train_basal[:,:valid_size]\n",
    "        self.valid_initIsc1 = train_initIsc1[:,:valid_size]\n",
    "        self.valid_initIsc2 = train_initIsc2[:, :valid_size]\n",
    "        self.valid_initIp = train_initIp[:, :valid_size]\n",
    "        self.valid_BW = train_BW[:, :valid_size]\n",
    "        self.valid_u2ss = train_u2ss[:, :valid_size]\n",
    "\n",
    "\n",
    "\n",
    "        self.train_x = train_x[:,valid_size:]\n",
    "        self.train_ins = train_ins[:,valid_size:]\n",
    "        self.train_meal = train_meal[:,valid_size:]\n",
    "        self.train_y = train_y[:,valid_size:]\n",
    "        self.train_basal = train_basal[:,valid_size:]\n",
    "        self.train_initIsc1 = train_initIsc1[:, valid_size:]\n",
    "        self.train_initIsc2 = train_initIsc2[:, valid_size:]\n",
    "        self.train_initIp = train_initIp[:, valid_size:]\n",
    "        self.train_BW = train_BW[:, valid_size:]\n",
    "        self.train_u2ss = train_u2ss[:, valid_size:]\n",
    "\n",
    "        self.test_x = test_x\n",
    "        self.test_ins = test_ins\n",
    "        self.test_meal = test_meal\n",
    "        self.test_y = test_y\n",
    "        self.test_basal = test_basal\n",
    "        self.test_initIsc1 = test_initIsc1\n",
    "        self.test_initIsc2 = test_initIsc2\n",
    "        self.test_initIp = test_initIp\n",
    "        self.test_BW = test_BW\n",
    "        self.test_u2ss = test_u2ss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    \"\"\"\n",
    "    A class used to represent a Recurrent Neural Network (GRU).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    hidden_size : int\n",
    "        The number of hidden units in the GR.\n",
    "    vocab_size : int\n",
    "        The size of the vocabulary used by the GRU.\n",
    "    sequence_length : int\n",
    "        The length of the input sequences fed to the GRU.\n",
    "    self.learning_rate : float\n",
    "        The learning rate used during training.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__(hidden_size, vocab_size, sequence_length, self.learning_rate)\n",
    "        Initializes an instance of the GRU class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, vocab_size, sequence_length, learning_rate):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the GRU class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_size : int\n",
    "            The number of hidden units in the GRU.\n",
    "        vocab_size : int\n",
    "            The size of the vocabulary used by the GRU.\n",
    "        sequence_length : int\n",
    "            The length of the input sequences fed to the GRU.\n",
    "        learning_rate : float\n",
    "            The learning rate used during training.\n",
    "        \"\"\"\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # model parameters\n",
    "        self.Wz = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.bz = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wr = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.br = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wa = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wy = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (vocab_size, hidden_size))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "\n",
    "        # initialize gradients for each parameter\n",
    "        self.dWz, self.dWr, self.dWa, self.dWy = np.zeros_like(self.Wz), np.zeros_like(self.Wr), np.zeros_like(\n",
    "            self.Wa), np.zeros_like(self.Wy)\n",
    "        self.dbz, self.dbr, self.dba, self.dby = np.zeros_like(self.bz), np.zeros_like(self.br), np.zeros_like(\n",
    "            self.bz), np.zeros_like(self.by)\n",
    "\n",
    "        # initialize parameters for adamw optimizer\n",
    "        self.mWz = np.zeros_like(self.Wz)\n",
    "        self.vWz = np.zeros_like(self.Wz)\n",
    "        self.mWr = np.zeros_like(self.Wr)\n",
    "        self.vWr = np.zeros_like(self.Wr)\n",
    "        self.mWa = np.zeros_like(self.Wa)\n",
    "        self.vWa = np.zeros_like(self.Wa)\n",
    "        self.mWy = np.zeros_like(self.Wy)\n",
    "        self.vWy = np.zeros_like(self.Wy)\n",
    "        self.mbz = np.zeros_like(self.bz)\n",
    "        self.vbz = np.zeros_like(self.bz)\n",
    "        self.mbr = np.zeros_like(self.br)\n",
    "        self.vbr = np.zeros_like(self.br)\n",
    "        self.mba = np.zeros_like(self.ba)\n",
    "        self.vba = np.zeros_like(self.ba)\n",
    "        self.mby = np.zeros_like(self.by)\n",
    "        self.vby = np.zeros_like(self.by)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Computes the sigmoid activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the sigmoid activation values.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the softmax activation values.\n",
    "        \"\"\"\n",
    "        # shift the input to prevent overflow when computing the exponentials\n",
    "        x = x - np.max(x)\n",
    "        # compute the exponentials of the shifted input\n",
    "        p = np.exp(x)\n",
    "        # normalize the exponentials by dividing by their sum\n",
    "        return p / np.sum(p)\n",
    "\n",
    "    def forward(self, X, c_prev, a_prev):\n",
    "        \"\"\"\n",
    "        Performs forward propagation for a simple GRU model.\n",
    "\n",
    "        Args:\n",
    "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
    "            c_prev (numpy array): Previous cell state, shape (hidden_size, 1)\n",
    "            a_prev (numpy array): Previous hidden state, shape (hidden_size, 1)\n",
    "\n",
    "        Returns: X (numpy array): Input sequence, shape (sequence_length, input_size) c (dictionary): Cell state for\n",
    "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) r (dictionary): Reset gate for\n",
    "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) z (dictionary): Update gate for\n",
    "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) cc (dictionary): Candidate cell\n",
    "        state for each time step, keys = time step, values = numpy array shape (hidden_size, 1) a (dictionary):\n",
    "        Hidden state for each time step, keys = time step, values = numpy array shape (hidden_size, 1) y_pred (\n",
    "        dictionary): Output probability vector for each time step, keys = time step, values = numpy array shape (\n",
    "        output_size, 1)\n",
    "        \"\"\"\n",
    "        # print('x shape:', X.shape)\n",
    "        # print('c_prevx shape:', c_prev.shape)\n",
    "        # print('a_prevx shape:', a_prev.shape)\n",
    "        # initialize dictionaries for backpropagation\n",
    "        # initialize dictionaries for backpropagation\n",
    "        r, z, c, cc, a, y_pred = {}, {}, {}, {}, {}, {}\n",
    "        c[-1] = np.copy(c_prev)  # store the initial cell state in the dictionary\n",
    "        a[-1] = np.copy(a_prev)  # store the initial hidden state in the dictionary\n",
    "\n",
    "        # iterate over each time step in the input sequence\n",
    "        for t in range(X.shape[0]):\n",
    "            # concatenate the input and hidden state\n",
    "            xt = X[t, :].reshape(-1, 1)\n",
    "            concat = np.vstack((a[t - 1], xt))\n",
    "            \n",
    "            wr_update = run_kernel(self.Wr, concat)\n",
    "            \n",
    "            # compute the reset gate\n",
    "            r[t] = self.sigmoid(wr_update + self.br)\n",
    "            # print('concat:', concat.shape)\n",
    "            # compute the update gate\n",
    "            z[t] = self.sigmoid(wr_update + self.bz)\n",
    "\n",
    "            # compute the candidate cell state\n",
    "            cc[t] = np.tanh(np.dot(self.Wa, np.vstack((r[t] * a[t - 1], xt))) + self.ba)\n",
    "\n",
    "            # compute the cell state\n",
    "            c[t] = z[t] * cc[t] + (1 - z[t]) * c[t - 1]\n",
    "\n",
    "            # compute the hidden state\n",
    "            a[t] = c[t]\n",
    "\n",
    "            \n",
    "            # compute the output probability vector\n",
    "            y_pred[t] = self.softmax(np.dot(self.Wy, a[t]) + self.by)\n",
    "            # print('y_predict:', y_pred[t].shape)\n",
    "            # print('a[t]:', a[t].shape)\n",
    "            # print('self.Wy:', self.Wy.shape)\n",
    "            # print('y_pred[t]:', y_pred[t].shape)\n",
    "\n",
    "        # return the output probability vectors, cell state, hidden state and gate vectors\n",
    "        return X, r, z, c, cc, a, y_pred\n",
    "\n",
    "    def backward(self, X, a_prev, c_prev, r, z, c, cc, a, y_pred, targets):\n",
    "        \"\"\"\n",
    "        Performs backward propagation through time for a GRU network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
    "            a_prev (numpy array): Previous hidden state, shape (hidden_size, 1)\n",
    "            r (dictionary): Reset gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            z (dictionary): Update gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            c (dictionary): Cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            cc (dictionary): Candidate cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            a (dictionary): Hidden state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            y_pred (dictionary): Output probability vector for each time step, keys = time step, values = numpy array shape (output_size, 1)\n",
    "            targets (numpy array): Target outputs for each time step, shape (sequence_length, output_size)\n",
    "\n",
    "        Returns:\n",
    "            None       \n",
    "        \"\"\"\n",
    "        # Initialize gradients for hidden state\n",
    "        dc_next = np.zeros_like(c_prev)\n",
    "        da_next = np.zeros_like(a_prev)\n",
    "\n",
    "        # Iterate backwards through time steps\n",
    "        for t in reversed(range(X.shape[0])):\n",
    "            # compute the gradient of the output probability vector\n",
    "            dy = np.copy(y_pred[t])\n",
    "            dy[targets[t]] -= 1\n",
    "\n",
    "            # compute the gradient of the output layer weights and biases\n",
    "            self.dWy += np.dot(dy, a[t].T)\n",
    "            self.dby += dy\n",
    "\n",
    "            # compute the gradient of the hidden state\n",
    "            da = np.dot(self.Wy.T, dy) + da_next\n",
    "\n",
    "            # compute the gradient of the update gate\n",
    "            xt = X[t, :].reshape(-1, 1)\n",
    "            concat = np.vstack((a_prev, xt))\n",
    "            dz = da * (a[t] - c[t])\n",
    "            self.dWz += np.dot(dz, concat.T)\n",
    "            self.dbz += dz\n",
    "\n",
    "            # compute the gradient of the reset gate\n",
    "            dr = da * np.dot(self.Wz[:, :self.hidden_size].T, dz) * (1 - r[t]) * r[t]\n",
    "            self.dWr += np.dot(dr, concat.T)\n",
    "            self.dbr += dr\n",
    "\n",
    "            # compute the gradient of the current hidden state\n",
    "            da = np.dot(self.Wa[:, :self.hidden_size].T, dr) + np.dot(self.Wz[:, :self.hidden_size].T, dz)\n",
    "            self.dWa += np.dot(da * (1 - a[t]**2), concat.T)\n",
    "            self.dba += da * (1 - a[t]**2)\n",
    "\n",
    "            # compute the gradient of the input to the next hidden state\n",
    "            da_next = np.dot(self.Wr[:, :self.hidden_size].T, dr) \\\n",
    "                      + np.dot(self.Wz[:, :self.hidden_size].T, dz) \\\n",
    "                      + np.dot(self.Wa[:, :self.hidden_size].T, da)\n",
    "        # clip gradients to avoid exploding gradients\n",
    "        for grad in [self.dWz, self.dWr, self.dWa, self.dWy, self.dbz, self.dbr, self.dba, self.dby]:\n",
    "            np.clip(grad, -1, 1)\n",
    "\n",
    "    # def loss(self, y_preds, targets):\n",
    "    #     \"\"\"\n",
    "    #     Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets.\n",
    "\n",
    "    #     Parameters:\n",
    "    #         y_preds (ndarray): Array of shape (sequence_length, vocab_size) containing the predicted probabilities for each time step.\n",
    "    #         targets (ndarray): Array of shape (sequence_length, 1) containing the true targets for each time step.\n",
    "\n",
    "    #     Returns:\n",
    "    #         float: Cross-entropy loss.\n",
    "    #     \"\"\"\n",
    "    #     # calculate cross-entropy loss\n",
    "\n",
    "    #     result = sum(-np.log(y_preds[t][targets[t], 0]) for t in range(self.sequence_length))\n",
    "    #     print('result:', result.shape)\n",
    "\n",
    "\n",
    "    #     return sum(-np.log(y_preds[t][targets[t], 0]) for t in range(self.sequence_length))\n",
    "\n",
    "    \n",
    "    def loss(self, y_preds, targets, y_initIsc1, y_initIsc2, y_initIp, train_u2ss, train_basal, train_ins, train_meal):\n",
    "        # har_data_instance = HarData()  # Create an instance of HarData\n",
    "\n",
    "        # # Convert 3D array self.y_initIsc1 to 2D by taking the first slice along the first axis\n",
    "        # self.y_initIsc1 = har_data_instance.train_initIsc1[0, :, :]\n",
    "        # # print('y_initIsc1 (converted to 2D):', self.y_initIsc1.shape)\n",
    "\n",
    "        # # Convert 3D array self.y_initIsc2 to 2D by taking the first slice along the first axis\n",
    "        # self.y_initIsc2 = har_data_instance.train_initIsc2[0, :, :]\n",
    "        # # print('y_initIsc2 (converted to 2D):', self.y_initIsc2.shape)\n",
    "\n",
    "        # # Convert 3D array self.y_initIp to 2D by taking the first slice along the first axis\n",
    "        # self.y_initIp = har_data_instance.train_initIp[0, :, :]\n",
    "        # # print('y_initIp (converted to 2D):', self.y_initIp.shape)\n",
    "\n",
    "        # # Convert 3D array self.train_u2ss to 2D by taking the first slice along the first axis\n",
    "        # self.train_u2ss = har_data_instance.train_u2ss[0, :, :]\n",
    "        # # print('train_u2ss (converted to 2D):', self.train_u2ss.shape)\n",
    "\n",
    "        # # Convert 3D array self.train_basal to 2D by taking the first slice along the first axis\n",
    "        # self.train_basal = har_data_instance.train_basal[0, :, :]\n",
    "        # # print('train_basal (converted to 2D):', self.train_basal.shape)\n",
    "\n",
    "        # # Convert 3D array self.train_ins to 2D by taking the first slice along the first axis\n",
    "        # self.train_ins = har_data_instance.train_ins[0, :, :]\n",
    "        # # print('train_ins (converted to 2D):', self.train_ins.shape)\n",
    "\n",
    "        # # Convert 3D array self.train_meal to 2D by taking the first slice along the first axis\n",
    "        # self.train_meal = har_data_instance.train_meal[0, :, :]\n",
    "        # # print('train_meal (converted to 2D):', self.train_meal.shape)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        ############################# Parameters ##################################\n",
    "        maxChange = 50\n",
    "        kempt = (1 + (0.5 - y_preds[0]) * maxChange / 100) * 0.18\n",
    "        kabs = (1 + (0.5 - y_preds[1]) * maxChange / 100) * 0.012\n",
    "        f = 0.9\n",
    "        Gb = (1 + (0.5 - y_preds[ 2]) * maxChange / 100) * 119.13\n",
    "        SG = (1 + (0.5 - y_preds[3]) * maxChange / 100) * 0.025\n",
    "        Vg = 1.45\n",
    "        p2 = (1 + (0.5 - y_preds[4]) * maxChange / 100) * 0.012\n",
    "        SI = (1 + (0.5 - y_preds[5]) * maxChange / 100) * 0.001035 / Vg\n",
    "        Ipb = (1 + (0.5 - y_preds[6]) * maxChange / 100)\n",
    "        alpha = 7\n",
    "        kd = (1 + (0.5 - y_preds[7]) * maxChange / 100) * 0.026\n",
    "        beta = np.floor((1 + (0.5 - y_preds[9]) * maxChange / 100) * 15)\n",
    "        Vi = 0.126\n",
    "        ka2 = (1 + (0.5 - y_preds[8]) * maxChange / 100) * 0.014\n",
    "        ke = 0.127\n",
    "        bolusD = np.floor((1 + (0.5 - y_preds[10]) * maxChange / 100) * 5)\n",
    "        r2 = 0.8124\n",
    "\n",
    "        # print(\"vals {}\".format(kempt))\n",
    "        # print(\"vals {}\".format(kabs))\n",
    "        # print(\"vals {}\".format(Gb))\n",
    "        # print(\"vals {}\".format(SG))\n",
    "        # print(\"vals {}\".format(p2))\n",
    "\n",
    "        ###########################################################################\n",
    "        # Initialize GVal as a 1D array (from the first element of targets)\n",
    "        target_array = np.array(targets)\n",
    "        GVal = target_array            # 2D array from 3D, removed the third dimension\n",
    "        # print('GVal:', GVal.shape)\n",
    "        pmVal = np.zeros(GVal.shape, dtype=np.float32)\n",
    "        Isc1Val = y_initIsc1[:, 0]      # Keep 2D, removed third dimension\n",
    "        Isc2Val = y_initIsc2[:, 0]      # Keep 2D, removed third dimension\n",
    "        IpVal = y_initIp[:, 0]          # Keep 2D, removed third dimension\n",
    "        Qsto1Val = np.zeros(GVal.shape, dtype=np.float32)\n",
    "        Ipb = np.zeros(GVal.shape, dtype=np.float32)\n",
    "        Qsto2Val = np.zeros(GVal.shape, dtype=np.float32)\n",
    "        QgutVal = np.zeros(GVal.shape, dtype=np.float32)\n",
    "        RatVal = np.zeros(GVal.shape, dtype=np.float32)\n",
    "        insVal = np.zeros(GVal.shape, dtype=np.float32)\n",
    "        xVal = np.zeros(GVal.shape, dtype=np.float32)\n",
    "        gVal = target_array            # 2D array from 3D, removed the third dimension\n",
    "        y_ins = np.zeros(GVal.shape, dtype=np.float32)\n",
    "        meal = np.zeros(GVal.shape, dtype=np.float32)\n",
    "\n",
    "        # No need for np.expand_dims since everything is 1D now\n",
    "\n",
    "\n",
    "        limitLoop = 43\n",
    "        tau = 1\n",
    "        stableEps = 100000.0\n",
    "\n",
    "        kempt_expanded = np.tile(kd, (44 // 13 + 1, 1))[:44, :]\n",
    "        p2_expanded = np.tile(kd, (44 // 13 + 1, 1))[:44, :]\n",
    "        SI_expanded = np.tile(kd, (44 // 13 + 1, 1))[:44, :]\n",
    "\n",
    "        for i in range(1, limitLoop):\n",
    "            ka1 = 0.0\n",
    "            \n",
    "            # For 1D arrays, adjust the indexing and remove the second and third dimensions\n",
    "            kd_expanded = np.tile(kd, (44 // 13 + 1, 1))[:44, :]\n",
    "            # print('kd:', kd.shape)\n",
    "            # print('ks:', ke)\n",
    "            Ipb = (kd_expanded / ke * train_u2ss[i-1]) / kd_expanded\n",
    "            # shape1 = GVal[i-1].shape\n",
    "\n",
    "            # Update the logic to work with 1D arrays\n",
    "            D = np.where((GVal[i-1] >= 60.0) & (GVal[i-1] < 119.13), 1.0, 0.0)\n",
    "            E = np.where(GVal[i-1] < 60.0, 1.0, 0.0)\n",
    "            \n",
    "            risk = np.abs(\n",
    "                10 * np.square(np.power(np.log(GVal[i - 1]), r2) - np.power(np.log(119.13), r2)) * D + \n",
    "                10 * np.power(np.power(np.log(60), r2) - np.power(np.log(119.13), r2), 2) * E\n",
    "            )\n",
    "\n",
    "\n",
    "            # Update all state variables to 1D\n",
    "            dummyIsc1 = Isc1Val[i-1] + tau * (-kd_expanded * Isc1Val[i-1] + (train_basal[i-1] + train_ins[i-1]) / Vi)\n",
    "            dummyIsc2 = Isc2Val[i-1] + tau * (kd * Isc1Val[i-1] - ka2 * Isc2Val[i-1])\n",
    "            dummyIp = IpVal[i-1] + tau * (ka2 * Isc2Val[i-1] - ke * IpVal[i-1])\n",
    "            dummyQsto1 = Qsto1Val[i-1] + tau * (-kempt_expanded * Qsto1Val[i-1] + train_meal[i-1])\n",
    "            dummyQsto2 = Qsto2Val[i-1] + tau * (kempt * Qsto1Val[i-1] - kempt * Qsto2Val[i-1])\n",
    "            dummyQgut = QgutVal[i-1] + tau * (kempt * Qsto2Val[i-1] - kabs * QgutVal[i-1])\n",
    "            \n",
    "            RatVal = f * kabs * QgutVal[i-1]\n",
    "            dummyXVal = xVal[i-1] + tau * (-p2_expanded * xVal[i-1] - SI_expanded * (IpVal[i-1] - Ipb))\n",
    "            dummygVal = gVal[i-1] + tau * (-(SG + risk * xVal[i-1]) * gVal[i-1] + SG * Gb + RatVal / Vg)\n",
    "\n",
    "            dummyG1 = GVal[i-1] + tau * (-(1 / alpha) * (GVal[i-1] - gVal[i-1]))\n",
    "            diffDummy = dummyG1 - GVal[i-1]\n",
    "            sumDiff = np.sum(np.square(diffDummy)) / 256 - stableEps\n",
    "            dummyG1 = np.where(sumDiff > 0.0, GVal[i-1], dummyG1)\n",
    "\n",
    "            # Append new values to the corresponding arrays\n",
    "            Isc1Val = np.append(Isc1Val, dummyIsc1)\n",
    "            Isc2Val = np.append(Isc2Val, dummyIsc2)\n",
    "            IpVal = np.append(IpVal, dummyIp)\n",
    "            Qsto1Val = np.append(Qsto1Val, dummyQsto1)\n",
    "            Qsto2Val = np.append(Qsto2Val, dummyQsto2)\n",
    "            QgutVal = np.append(QgutVal, dummyQgut)\n",
    "            gVal = np.append(gVal, dummygVal)\n",
    "            xVal = np.append(xVal, dummyXVal)\n",
    "            GVal = np.append(GVal, dummyG1)\n",
    "\n",
    "        # Calculate error over the loop\n",
    "        err = np.sqrt(np.mean(np.square(targets[limitLoop] - GVal)))\n",
    "        print('err:', err)\n",
    "\n",
    "#         # Break the loop and return the error if it's below the threshold\n",
    "#         if err < 3.2:\n",
    "#             print('err:', err)\n",
    "#             return err\n",
    "\n",
    "        return err\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4):\n",
    "        \"\"\"\n",
    "        Updates the GRU's parameters using the AdamW optimization algorithm.\n",
    "        \"\"\"\n",
    "        \n",
    "        # AdamW update for Wz\n",
    "        self.mWz = beta1 * self.mWz + (1 - beta1) * self.dWz\n",
    "        self.vWz = beta2 * self.vWz + (1 - beta2) * np.square(self.dWz)\n",
    "        m_hat = self.mWz / (1 - beta1)\n",
    "        v_hat = self.vWz / (1 - beta2)\n",
    "        self.Wz -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wz)\n",
    "\n",
    "        # AdamW update for bu\n",
    "        self.mbz = beta1 * self.mbz + (1 - beta1) * self.dbz\n",
    "        self.vbz = beta2 * self.vbz + (1 - beta2) * np.square(self.dbz)\n",
    "        m_hat = self.mbz / (1 - beta1)\n",
    "        v_hat = self.vbz / (1 - beta2)\n",
    "        self.bz -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.bz)\n",
    "\n",
    "        # AdamW update for Wr\n",
    "        self.mWr = beta1 * self.mWr + (1 - beta1) * self.dWr\n",
    "        self.vWr = beta2 * self.vWr + (1 - beta2) * np.square(self.dWr)\n",
    "        m_hat = self.mWr / (1 - beta1)\n",
    "        v_hat = self.vWr / (1 - beta2)\n",
    "        self.Wr -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wr)\n",
    "\n",
    "        # AdamW update for br\n",
    "        self.mbr = beta1 * self.mbr + (1 - beta1) * self.dbr\n",
    "        self.vbr = beta2 * self.vbr + (1 - beta2) * np.square(self.dbr)\n",
    "        m_hat = self.mbr / (1 - beta1)\n",
    "        v_hat = self.vbr / (1 - beta2)\n",
    "        self.br -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.br)\n",
    "\n",
    "        # AdamW update for Wa\n",
    "        self.mWa = beta1 * self.mWa + (1 - beta1) * self.dWa\n",
    "        self.vWa = beta2 * self.vWa + (1 - beta2) * np.square(self.dWa)\n",
    "        m_hat = self.mWa / (1 - beta1)\n",
    "        v_hat = self.vWa / (1 - beta2)\n",
    "        self.Wa -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wa)\n",
    "\n",
    "        # AdamW update for br\n",
    "        self.mba = beta1 * self.mba + (1 - beta1) * self.dba\n",
    "        self.vba = beta2 * self.vba + (1 - beta2) * np.square(self.dba)\n",
    "        m_hat = self.mba / (1 - beta1)\n",
    "        v_hat = self.vba / (1 - beta2)\n",
    "        self.ba -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.ba)\n",
    "\n",
    "        # AdamW update for Wy\n",
    "        self.mWy = beta1 * self.mWy + (1 - beta1) * self.dWy\n",
    "        self.vWy = beta2 * self.vWy + (1 - beta2) * np.square(self.dWy)\n",
    "        m_hat = self.mWy / (1 - beta1)\n",
    "        v_hat = self.vWy / (1 - beta2)\n",
    "        self.Wy -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wy)\n",
    "\n",
    "        # AdamW update for by\n",
    "        self.mby = beta1 * self.mby + (1 - beta1) * self.dby\n",
    "        self.vby = beta2 * self.vby + (1 - beta2) * np.square(self.dby)\n",
    "        m_hat = self.mby / (1 - beta1)\n",
    "        v_hat = self.vby / (1 - beta2)\n",
    "        self.by -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.by)\n",
    "\n",
    "    def train(self, data_generator,iterations):\n",
    "        \"\"\"\n",
    "        Train the GRU on a dataset using backpropagation through time.\n",
    "\n",
    "        Args:\n",
    "            data_generator: An instance of DataGenerator containing the training data.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        iter_num = 0\n",
    "        # stopping criterion for training\n",
    "        threshold = 50\n",
    "    \n",
    "        smooth_loss = -np.log(1.0 / data_generator.vocab_size) * self.sequence_length  # initialize loss\n",
    "\n",
    "        har_data_instance = HarData()  # Create an instance of HarData\n",
    "\n",
    "        # Convert 3D array self.y_initIsc1 to 2D by taking the first slice along the first axis\n",
    "        self.y_initIsc1 = har_data_instance.train_initIsc1[0, :, :]\n",
    "        # print('y_initIsc1 (converted to 2D):', self.y_initIsc1.shape)\n",
    "\n",
    "        # Convert 3D array self.y_initIsc2 to 2D by taking the first slice along the first axis\n",
    "        self.y_initIsc2 = har_data_instance.train_initIsc2[0, :, :]\n",
    "        # print('y_initIsc2 (converted to 2D):', self.y_initIsc2.shape)\n",
    "\n",
    "        # Convert 3D array self.y_initIp to 2D by taking the first slice along the first axis\n",
    "        self.y_initIp = har_data_instance.train_initIp[0, :, :]\n",
    "        # print('y_initIp (converted to 2D):', self.y_initIp.shape)\n",
    "\n",
    "        # Convert 3D array self.train_u2ss to 2D by taking the first slice along the first axis\n",
    "        self.train_u2ss = har_data_instance.train_u2ss[0, :, :]\n",
    "        # print('train_u2ss (converted to 2D):', self.train_u2ss.shape)\n",
    "\n",
    "        # Convert 3D array self.train_basal to 2D by taking the first slice along the first axis\n",
    "        self.train_basal = har_data_instance.train_basal[0, :, :]\n",
    "        # print('train_basal (converted to 2D):', self.train_basal.shape)\n",
    "\n",
    "        # Convert 3D array self.train_ins to 2D by taking the first slice along the first axis\n",
    "        self.train_ins = har_data_instance.train_ins[0, :, :]\n",
    "        # print('train_ins (converted to 2D):', self.train_ins.shape)\n",
    "\n",
    "        # Convert 3D array self.train_meal to 2D by taking the first slice along the first axis\n",
    "        self.train_meal = har_data_instance.train_meal[0, :, :]\n",
    "        # print('train_meal (converted to 2D):', self.train_meal.shape)\n",
    "        \n",
    "        early_stop_patience = 3  # The number of consecutive iterations with small loss change\n",
    "        consecutive_small_loss_change = 0  # Counter for consecutive small loss changes\n",
    "        threshold = 0.10  # 10% threshold for loss change\n",
    "        prev_loss = None  # To store the previous loss value\n",
    "\n",
    "        while (iter_num < iterations):\n",
    "            # initialize hidden state at the beginning of each sequence\n",
    "            if data_generator.pointer == 0:\n",
    "                c_prev = np.zeros((self.hidden_size, 1))\n",
    "                a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "            # get a batch of inputs and targets\n",
    "            inputs, targets = data_generator.next_batch()\n",
    "\n",
    "            # forward pass\n",
    "            X, r, z, c, cc, a, y_pred = self.forward(inputs, c_prev, a_prev)\n",
    "            # print('y_pred_list:', len(y_pred))\n",
    "            # print('y_pred_list:', y_pred[199].shape)\n",
    "\n",
    "            # backward pass\n",
    "            self.backward(X, a_prev, c_prev, r, z, c, cc, a, y_pred, targets)\n",
    "\n",
    "            # calculate and update loss\n",
    "#             loss = self.loss(y_pred, targets, self.y_initIsc1, self.y_initIsc2, self.y_initIp, self.train_u2ss, self.train_basal, self.train_ins, self.train_meal)\n",
    "            # print('loss shape:', loss)\n",
    "            # print('loss shape:')\n",
    "            \n",
    "            loss =  np.sqrt(np.mean(np.square(np.array(list(y_pred.values())) - np.array(targets))))\n",
    "            print('loss:', loss)\n",
    "            \n",
    "            \n",
    "#              # Early stopping logic\n",
    "#             if prev_loss is not None:\n",
    "#                 # Calculate the percentage change in loss\n",
    "#                 loss_diff = abs(prev_loss - loss) / prev_loss\n",
    "\n",
    "#                 if loss_diff < threshold:  # If the loss change is less than 10%\n",
    "#                     consecutive_small_loss_change += 1\n",
    "#                     if consecutive_small_loss_change >= early_stop_patience:\n",
    "#                         print(f\"Early stopping triggered at iteration {iter_num} with loss: {loss}\")\n",
    "#                         break\n",
    "#                 else:\n",
    "#                     consecutive_small_loss_change = 0  # Reset the counter if the change is larger than 10%\n",
    "\n",
    "#             prev_loss = loss  # Update the previous loss\n",
    "            \n",
    "            self.adamw()\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "            # update previous hidden state for the next batch\n",
    "            a_prev = a[self.sequence_length - 1]\n",
    "            c_prev = c[self.sequence_length - 1]\n",
    "#             if iter_num == 5900 or iter_num == 30000:\n",
    "#                         self.learning_rate *= 0.1\n",
    "            # print progress every 100 iterations\n",
    "            if iter_num % 100 == 0:\n",
    "#                 self.learning_rate *= 0.99\n",
    "                sample_idx = self.sample(c_prev, a_prev, inputs[0, :], 200)\n",
    "                print(''.join(data_generator.idx_to_char[idx] for idx in sample_idx))\n",
    "                print(\"\\n\\niter :%d, loss:%f\" % (iter_num, smooth_loss))\n",
    "            iter_num += 1\n",
    "            \n",
    "        return smooth_loss, y_pred\n",
    "\n",
    "    def sample(self, c_prev, a_prev, seed_idx, n):\n",
    "        \"\"\"\n",
    "        Sample a sequence of integers from the model.\n",
    "\n",
    "        Args:\n",
    "            c_prev (numpy.ndarray): Previous cell state, a numpy array of shape (hidden_size, 1).\n",
    "            a_prev (numpy.ndarray): Previous hidden state, a numpy array of shape (hidden_size, 1).\n",
    "            seed_idx (numpy.ndarray): Seed letter from the first time step, a numpy array of shape (vocab_size, 1).\n",
    "            n (int): Number of characters to generate.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of integers representing the generated sequence.\n",
    "\n",
    "        \"\"\"\n",
    "        # initialize input and seed_idx\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        # convert one-hot encoding to integer index\n",
    "        seed_idx = np.argmax(seed_idx, axis=-1)\n",
    "\n",
    "        # set the seed letter as the input for the first time step\n",
    "        x[seed_idx] = 1\n",
    "\n",
    "        # generate sequence of characters\n",
    "        idxes = []\n",
    "        c = np.copy(c_prev)\n",
    "        a = np.copy(a_prev)\n",
    "        for t in range(n):\n",
    "            # compute the hidden state and cell state\n",
    "            concat = np.vstack((a, x))\n",
    "            z = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
    "            r = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
    "            cc = np.tanh(np.dot(self.Wa, np.vstack((r * a, x))) + self.ba)\n",
    "            c = z * c + (1 - z) * cc\n",
    "            a = c\n",
    "            # compute the output probabilities\n",
    "            y = self.softmax(np.dot(self.Wy, a) + self.by)\n",
    "\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y.ravel())\n",
    "\n",
    "            # set the input for the next time step\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            # append the sampled character to the sequence\n",
    "            idxes.append(idx)\n",
    "\n",
    "        # return the generated sequence\n",
    "        return idxes\n",
    "\n",
    "    def predict(self, data_generator, start, n):\n",
    "        \"\"\"\n",
    "        Generate a sequence of n characters using the trained GRU model, starting from the given start sequence.\n",
    "\n",
    "        Args:\n",
    "        - data_generator: an instance of DataGenerator\n",
    "        - start: a string containing the start sequence\n",
    "        - n: an integer indicating the length of the generated sequence\n",
    "\n",
    "        Returns:\n",
    "        - txt: a string containing the generated sequence\n",
    "        \"\"\"\n",
    "        # initialize input sequence\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        chars = [ch for ch in start]\n",
    "        idxes = []\n",
    "        for i in range(len(chars)):\n",
    "            idx = data_generator.char_to_idx[chars[i]]\n",
    "            # idx = data_generator[chars[i]]\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "        # initialize cell state and hidden state\n",
    "        a = np.zeros((self.hidden_size, 1))\n",
    "        c = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # generate new sequence of characters\n",
    "        for t in range(n):\n",
    "            # compute the hidden state and cell state\n",
    "            concat = np.vstack((a, x))\n",
    "\n",
    "            # compute the reset gate\n",
    "            r = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
    "\n",
    "            # compute the update gate\n",
    "            z = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
    "\n",
    "            # compute the candidate cell state\n",
    "            cc = np.tanh(np.dot(self.Wa, np.vstack((r * a, x))) + self.ba)\n",
    "\n",
    "            # compute the cell state\n",
    "            c = z * cc + (1 - z) * c\n",
    "\n",
    "            # compute the hidden state\n",
    "            a = c\n",
    "\n",
    "            # compute the output probability vector\n",
    "            y_pred = self.softmax(np.dot(self.Wy, a) + self.by)\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_pred.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "        txt = ''.join(data_generator.idx_to_char[i] for i in idxes)\n",
    "        return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Function to print current memory and CPU usage\"\"\"\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    cpu_usage = psutil.cpu_percent(interval=1)\n",
    "    \n",
    "    print(f\"Total memory: {memory_info.total / (1024 ** 2):.2f} MB\")\n",
    "    print(f\"Available memory: {memory_info.available / (1024 ** 2):.2f} MB\")\n",
    "    print(f\"Used memory: {memory_info.used / (1024 ** 2):.2f} MB\")\n",
    "    print(f\"Memory percent used: {memory_info.percent}%\")\n",
    "    print(f\"CPU usage: {cpu_usage}%\\n\")\n",
    "    \n",
    "def print_gpu_usage():\n",
    "    \"\"\"Function to print current GPU power usage (if using a GPU)\"\"\"\n",
    "    try:\n",
    "        gpu_power_usage = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv,noheader,nounits\"]\n",
    "        )\n",
    "        print(f\"GPU Power usage: {gpu_power_usage.decode('utf-8').strip()} W\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not fetch GPU power usage: {e}\")\n",
    "\n",
    "# Print memory usage before buffer allocation\n",
    "print(\"Memory usage before allocation:\")\n",
    "print_memory_usage()\n",
    "\n",
    "# Print GPU power usage if available\n",
    "print_gpu_usage()\n",
    "\n",
    "sequence_length = 50\n",
    "# Read text from the \"input.txt\" file\n",
    "data_generator = DataGenerator('Train_570_13_glucose.txt', sequence_length)\n",
    "gru = GRU(hidden_size=128, vocab_size=data_generator.vocab_size, sequence_length=sequence_length, learning_rate=0.001)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "smooth_loss, y_pred = gru.train(data_generator, iterations=32)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training took {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Print memory usage after training\n",
    "print(\"Memory usage after training:\")\n",
    "print_memory_usage()\n",
    "\n",
    "# Print GPU power usage again after training\n",
    "print_gpu_usage()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
